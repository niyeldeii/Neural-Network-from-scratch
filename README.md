# Neural Network from Scratch

Neural networks are inspired by the human brain and have been pivotal  in revolutionizing the fields of Artificial Intelligence (AI) and Machine Learning (ML). In the brain, neurons transmit signals through synapses, processing information in layers. Similarly, neural networks consist of layers of artificial neurons that work together to solve complex tasks, such as pattern recognition, decision-making, and prediction.

In AI, neural networks learn from data by adjusting the weights of connections between neurons. During training, the network minimizes the error in its predictions, improving over time through optimization.

This project presents a basic implementation of a neural network, demonstrating the core principles of learning, training, and prediction, which are fundamental to machine learning and AI systems.

## Description

This neural network is a basic implementation of a multi-layer perceptron (MLP) with a single hidden layer. It uses the **sigmoid** activation function and **mean squared error (MSE)** loss function to train the model. The training process involves using **gradient descent** to minimize the loss through backpropagation.

The project is designed to help understand the core workings of neural networks, from forward propagation to backpropagation and weight updates.

### Features:
- Feedforward and backpropagation algorithms
- Gradient descent for training
- Sigmoid activation function
- Mean squared error loss function

